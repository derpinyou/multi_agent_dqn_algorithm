{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8d82a-acb2-4a92-ad20-6fd1ebf66712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from market_places_env import MarketPlacesEnv\n",
    "\n",
    "# Инициализируем среду\n",
    "\n",
    "env_ = MarketPlacesEnv()\n",
    "env_.reset()\n",
    "\n",
    "# Зафиксируем размерность пространства действий и наблюдений\n",
    "# Для каждого вида агентов\n",
    "# А также их количество\n",
    "# И создадим пока пустой ключ для нейросетей\n",
    "\n",
    "dims_dict = {\n",
    "    'household': {\n",
    "        'agents_num':5, \n",
    "        'actions_dim':3,\n",
    "        'state_dim':33,\n",
    "        'networks':[],\n",
    "        'opt':[],\n",
    "        'rewards':[0] * 5,\n",
    "        'cum_rewards':[0] * 5,\n",
    "        'prev_state':[[]] * 5,\n",
    "        'actions':[[]] * 5\n",
    "    },\n",
    "    'marketplace': {\n",
    "        'agents_num':2,\n",
    "        'actions_dim':3,\n",
    "        'state_dim':19, \n",
    "        'networks':[],\n",
    "        'opt':[],\n",
    "        'rewards':[0] * 2,\n",
    "        'cum_rewards':[0] * 2,\n",
    "        'prev_state':[[]] * 2,\n",
    "        'actions':[[]] * 2\n",
    "    },\n",
    "    'distributor': {\n",
    "        'agents_num':1,\n",
    "        'actions_dim':3, \n",
    "        'state_dim':26,\n",
    "        'networks':[],\n",
    "        'opt':[],\n",
    "        'rewards':[0] * 1,\n",
    "        'cum_rewards':[0] * 1,\n",
    "        'prev_state':[[]] * 1,\n",
    "        'actions':[[]] * 1\n",
    "    },\n",
    "    'firm': {\n",
    "        'agents_num':2,\n",
    "        'actions_dim':3,\n",
    "        'state_dim':52,\n",
    "        'networks':[],\n",
    "        'opt':[],\n",
    "        'rewards':[0] * 2,\n",
    "        'cum_rewards':[0] * 2,\n",
    "        'prev_state':[[]] * 2,\n",
    "        'actions':[[]] * 2\n",
    "    },\n",
    "}\n",
    "\n",
    "# Инициализируем архитектуры сетей для каждого агента\n",
    "\n",
    "for kkey_ in dims_dict.keys():\n",
    "    instances_num = dims_dict[kkey_]['agents_num']\n",
    "    actions_dim = dims_dict[kkey_]['actions_dim']\n",
    "    state_dim = dims_dict[kkey_]['state_dim']\n",
    "    \n",
    "    for j in range(instances_num):\n",
    "        \n",
    "        network = nn.Sequential()\n",
    "        network.add_module('layer1', nn.Linear(state_dim, 32))\n",
    "        network.add_module('relu1', nn.ReLU())\n",
    "        network.add_module('layer2', nn.Linear(32, 32))\n",
    "        network.add_module('relu1', nn.ReLU())\n",
    "        network.add_module('layer2', nn.Linear(32, 32))\n",
    "        network.add_module('relu1', nn.ReLU())\n",
    "        network.add_module('layer2', nn.Linear(32, 32))\n",
    "        network.add_module('relu1', nn.ReLU())\n",
    "        network.add_module('layer2', nn.Linear(32, 32))\n",
    "        network.add_module('relu1', nn.ReLU())\n",
    "        network.add_module('layer3', nn.Linear(32, actions_dim))\n",
    "\n",
    "        dims_dict[kkey_]['networks'].append(network)\n",
    "\n",
    "        opt_ = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "        \n",
    "        dims_dict[kkey_]['opt'].append(opt_)\n",
    "\n",
    "def get_agent_action(network_, state, agent_type='household', agent_idx=0, epsilon=0.1):\n",
    "    state_ = torch.tensor(state, dtype=torch.float32)\n",
    "    if agent_idx != None:\n",
    "        network_ = dims_dict[agent_type]['networks'][agent_idx]\n",
    "    else: \n",
    "        network_ = dims_dict[agent_type]['networks'][0]\n",
    "\n",
    "    q_values_ = network_(state_).detach().numpy()\n",
    "\n",
    "    q_values_dict = {q_values_[i]: i for i in range(q_values_.shape[0])}\n",
    "    pos = np.argmax(q_values_)\n",
    "    val = q_values_[pos]\n",
    "\n",
    "    epsilon = 0.1\n",
    "    greedy_action = q_values_dict[val]\n",
    "    should_explore = np.random.binomial(n=1, p=epsilon)\n",
    "    \n",
    "    if should_explore:\n",
    "        chosen_action = np.random.choice(a = [0, 1, 2])\n",
    "    else:\n",
    "        chosen_action = greedy_action\n",
    "    \n",
    "    return chosen_action + 1\n",
    "\n",
    "sample_state = env_.get_marketplace_state()\n",
    "sample_network = dims_dict['marketplace']['networks'][0]\n",
    "sample_action = get_agent_action(sample_network, sample_state, agent_type='marketplace')\n",
    "sample_state, sample_action\n",
    "\n",
    "def compute_td_loss(network, states, actions, rewards, next_states, is_done, gamma=0.999, check_shapes=False):\n",
    "    \"\"\" Compute td loss using torch operations only \"\"\"\n",
    "    states = torch.tensor(states, dtype=torch.float32) \n",
    "    actions = torch.tensor(actions, dtype=torch.long) - 1\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "  \n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    is_done = torch.tensor(is_done, dtype=torch.uint8) \n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = network(states)\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[\n",
    "        range(states.shape[0]), actions\n",
    "    ]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = network(next_states)\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = torch.max(predicted_next_qvalues, dim=-1)[0]\n",
    "    assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
    "\n",
    "    # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    target_qvalues_for_actions = torch.where(\n",
    "        is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    return loss\n",
    "\n",
    "sample_reward = env_.marketplace_reward(agent_idx=0)\n",
    "sample_opt = dims_dict['marketplace']['opt'][0]\n",
    "\n",
    "sample_opt.zero_grad()\n",
    "compute_td_loss(sample_network, [sample_state], [sample_action], [10], [sample_state], [False]).backward()\n",
    "sample_opt.step()\n",
    "\n",
    "def generate_session(env, train=True):\n",
    "    total_reward = 0\n",
    "    rewards = {}\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Действия маркетплейсов на Шаге 1\n",
    "        for mm_ind in range(dims_dict['marketplace']['agents_num']):\n",
    "            mm_state = env.get_marketplace_state(agent_idx=mm_ind)\n",
    "            dims_dict['marketplace']['prev_state'][mm_ind] = dims_dict['marketplace']['prev_state'][mm_ind] + [mm_state]\n",
    "            network_ind = dims_dict['marketplace']['networks'][mm_ind]\n",
    "            mm_action = get_agent_action(network_ind, mm_state, agent_type='marketplace', agent_idx=mm_ind)\n",
    "            dims_dict['marketplace']['actions'][mm_ind] = dims_dict['marketplace']['actions'][mm_ind] + [mm_action]\n",
    "            env.marketplace_step(mm_action, agent_idx=mm_ind)\n",
    "            \n",
    "\n",
    "        # Действие дистрибьютора на Шаге 2\n",
    "        d_state = env.get_distributor_state()\n",
    "        dims_dict['distributor']['prev_state'][0] = dims_dict['distributor']['prev_state'][0] + [d_state]\n",
    "        network_ind = dims_dict['distributor']['networks'][0]\n",
    "        d_action = get_agent_action(network_ind, d_state, agent_type='distributor', agent_idx=None)\n",
    "        dims_dict['distributor']['actions'][0] = dims_dict['distributor']['actions'][0] + [d_action]\n",
    "        env.distributor_step(d_action)\n",
    "\n",
    "        # Действие домохозяйства для выбора фирмы на Шаге 3\n",
    "        for h_ind in range(dims_dict['household']['agents_num']):\n",
    "            \n",
    "            h_state = env.get_household_state(agent_idx=h_ind, regime='choose_firm')            \n",
    "            dims_dict['household']['prev_state'][h_ind] = dims_dict['household']['prev_state'][h_ind] + [h_state]\n",
    "            network_ind = dims_dict['household']['networks'][h_ind]\n",
    "            h_action = get_agent_action(network_ind, h_state, agent_type='household', agent_idx=h_ind)\n",
    "            dims_dict['household']['actions'][h_ind] = dims_dict['household']['actions'][h_ind] + [h_action]\n",
    "            env.household_step(agent_idx=h_ind, regime='choose_firm', action_=h_action)\n",
    "\n",
    "        # Действие по выплате зарплат и пересчет премий на Шаге 4\n",
    "        for f_ind in range(dims_dict['firm']['agents_num']):\n",
    "            f_state = env.get_firm_state(agent_idx=f_ind, regime='wage')\n",
    "            dims_dict['firm']['prev_state'][f_ind] = dims_dict['firm']['prev_state'][f_ind] + [f_state]\n",
    "            network_ind = dims_dict['firm']['networks'][f_ind]\n",
    "            f_action = get_agent_action(network_ind, f_state, agent_type='firm', agent_idx=f_ind)\n",
    "            dims_dict['firm']['actions'][f_ind] = dims_dict['firm']['actions'][f_ind] + [f_action]\n",
    "            env.firm_step(agent_idx=f_ind, regime='wage', action_=f_action)\n",
    "            \n",
    "        # Действие производства товарова на Шаге 5\n",
    "        for f_ind in range(dims_dict['firm']['agents_num']):\n",
    "            f_state = env.get_firm_state(agent_idx=f_ind, regime='produce')\n",
    "            dims_dict['firm']['prev_state'][f_ind] = dims_dict['firm']['prev_state'][f_ind] + [f_state]\n",
    "            network_ind = dims_dict['firm']['networks'][f_ind]\n",
    "            f_action = get_agent_action(network_ind, f_state, agent_type='firm', agent_idx=f_ind)\n",
    "            dims_dict['firm']['actions'][f_ind] = dims_dict['firm']['actions'][f_ind] + [f_action]\n",
    "            env.firm_step(agent_idx=f_ind, regime='produce', action_=f_action)\n",
    "\n",
    "        # Действие изменения стратегии дистрибуции на Шаге 6\n",
    "        for f_ind in range(dims_dict['firm']['agents_num']):\n",
    "            f_state = env.get_firm_state(agent_idx=f_ind, regime='redistribute_inventories')\n",
    "            dims_dict['firm']['prev_state'][f_ind] = dims_dict['firm']['prev_state'][f_ind] + [f_state]\n",
    "            network_ind = dims_dict['firm']['networks'][f_ind]\n",
    "            f_action = get_agent_action(network_ind, f_state, agent_type='firm', agent_idx=f_ind)\n",
    "            dims_dict['firm']['actions'][f_ind] = dims_dict['firm']['actions'][f_ind] + [f_action]\n",
    "            env.firm_step(agent_idx=f_ind, regime='redistribute_inventories', action_=f_action)\n",
    "\n",
    "        # Действие обновления цен на Шаге 7\n",
    "        for f_ind in range(dims_dict['firm']['agents_num']):\n",
    "            for reg in ['prices_online_1', 'prices_online_2', 'price_offline']:\n",
    "                f_state = env.get_firm_state(agent_idx=f_ind, regime=reg)\n",
    "                dims_dict['firm']['prev_state'][f_ind] = dims_dict['firm']['prev_state'][f_ind] + [f_state]\n",
    "                network_ind = dims_dict['firm']['networks'][f_ind]\n",
    "                f_action = get_agent_action(network_ind, f_state, agent_type='firm', agent_idx=f_ind)\n",
    "                dims_dict['firm']['actions'][f_ind] = dims_dict['firm']['actions'][f_ind] + [f_action]\n",
    "                env.firm_step(agent_idx=f_ind, regime=reg, action_=f_action)\n",
    "\n",
    "        # Действие адаптации предпочтений на Шаге 8\n",
    "        for h_ind in range(dims_dict['household']['agents_num']):\n",
    "            h_state = env.get_household_state(agent_idx=h_ind, regime='redistribute_demand')\n",
    "            network_ind = dims_dict['household']['networks'][h_ind]\n",
    "            h_action = get_agent_action(network_ind, h_state, agent_type='household', agent_idx=h_ind)\n",
    "            dims_dict['household']['prev_state'][h_ind] = dims_dict['household']['prev_state'][h_ind] + [h_state]\n",
    "            dims_dict['household']['actions'][h_ind] = dims_dict['household']['actions'][h_ind] + [h_action]\n",
    "            env.household_step(agent_idx=h_ind, regime='redistribute_demand', action_=h_action)\n",
    "\n",
    "        # Расчитываем  награды на Шаге 9\n",
    "        for key_ in dims_dict.keys():\n",
    "            for ind in range(dims_dict[key_]['agents_num']):\n",
    "                if key_ == 'household':\n",
    "                    rew = env.household_reward(agent_idx=ind)\n",
    "                    dims_dict[key_]['cum_rewards'][ind] += rew\n",
    "                    dims_dict[key_]['rewards'][ind] = rew\n",
    "                    total_reward += rew\n",
    "                if key_ == 'marketpalce':\n",
    "                    rew = env.marketplace_reward(agent_idx=ind)\n",
    "                    dims_dict[key_]['cum_rewards'][ind] += rew\n",
    "                    dims_dict[key_]['rewards'][ind] = rew\n",
    "                    total_reward += rew\n",
    "                if key_ == 'firm':\n",
    "                    rew = env.firm_reward(agent_idx=ind)\n",
    "                    dims_dict[key_]['cum_rewards'][ind] += rew\n",
    "                    dims_dict[key_]['rewards'][ind] = rew\n",
    "                    total_reward += rew\n",
    "                if key_ == 'distributor':\n",
    "                    rew = env.distributor_reward()\n",
    "                    dims_dict[key_]['cum_rewards'][ind] += rew\n",
    "                    dims_dict[key_]['rewards'][ind] = rew\n",
    "                    total_reward += rew\n",
    "\n",
    "        if len(env.marketplace_stock) != 2:\n",
    "            print(\"WTF\")\n",
    "        # Обновляем стратегию RL-агентов\n",
    "        if train:\n",
    "            for key_ in dims_dict.keys():\n",
    "                for ind in range(dims_dict[key_]['agents_num']):\n",
    "                    for j in range(len(dims_dict[key_]['prev_state'][ind])):\n",
    "                        ind_opt = dims_dict[key_]['opt'][ind]\n",
    "                        ind_network = dims_dict[key_]['networks'][ind]\n",
    "                        ind_opt.zero_grad()\n",
    "                        \n",
    "                        if key_ == 'household':\n",
    "                            action__ = dims_dict[key_]['actions'][ind][j]\n",
    "                            regimes_dict = {\n",
    "                                0:'choose_firm',\n",
    "                                1:'redistribute_demand'\n",
    "                            }\n",
    "                            regime_ind = regimes_dict[j]\n",
    "                            cur_state = env.get_household_state(agent_idx=ind, regime=regime_ind)\n",
    "                            to_compare_state = dims_dict[key_]['prev_state'][ind][j]\n",
    "                            reward__ = env.household_reward(agent_idx=ind)\n",
    "                            compute_td_loss(ind_network, [to_compare_state], [action__], [reward__], [cur_state], [done]).backward()\n",
    "                        \n",
    "                        if key_ == 'marketpalce':\n",
    "                            action__ = dims_dict[key_]['actions'][ind][j]\n",
    "                            cur_state = env.get_marketplace_state(agent_idx=ind)\n",
    "                            to_compare_state = dims_dict[key_]['prev_state'][ind][j]\n",
    "                            reward__ = env.marketplace_reward(agent_idx=ind)\n",
    "                            compute_td_loss(ind_network, [to_compare_state], [action__], [reward__], [cur_state], [done]).backward()\n",
    "\n",
    "                        if key_ == 'firm':\n",
    "                            regimes_dict = {\n",
    "                                0:'wage',\n",
    "                                1:'produce',\n",
    "                                2:'redistribute_inventories',\n",
    "                                3:'prices_online_1',\n",
    "                                4:'prices_online_2',\n",
    "                                5:'price_offline'\n",
    "                            }\n",
    "                            regime_ind = regimes_dict[j]\n",
    "                            cur_state = env.get_firm_state(agent_idx=ind, regime=regime_ind)\n",
    "                            to_compare_state = dims_dict[key_]['prev_state'][ind][j]\n",
    "                            reward__ = env.firm_reward(agent_idx=ind)\n",
    "                            compute_td_loss(ind_network, [to_compare_state], [action__], [reward__], [cur_state], [done]).backward()\n",
    "                            \n",
    "                        if key_ == 'distributor':\n",
    "                            action__ = dims_dict[key_]['actions'][ind][j]\n",
    "                            reward__ = env.distributor_reward()\n",
    "                            cur_state = env.get_distributor_state()\n",
    "                            to_compare_state = dims_dict[key_]['prev_state'][ind][j]\n",
    "                            compute_td_loss(ind_network, [to_compare_state], [action__], [reward__], [cur_state], [done]).backward()\n",
    "                        ind_opt.step() \n",
    "            \n",
    "        # Сброс истории действий и промежуточных состояний\n",
    "        dims_dict['household']['actions'] = [[]] * 5\n",
    "        dims_dict['household']['prev_state'] = [[]] * 5\n",
    "\n",
    "        dims_dict['marketplace']['actions'] = [[]] * 2\n",
    "        dims_dict['marketplace']['prev_state'] = [[]] * 2\n",
    "\n",
    "        dims_dict['distributor']['actions'] = [[]] * 1\n",
    "        dims_dict['distributor']['prev_state'] = [[]] * 1\n",
    "\n",
    "        dims_dict['firm']['actions'] = [[]] * 2\n",
    "        dims_dict['firm']['prev_state'] = [[]] * 2\n",
    "        \n",
    "        # Запускаем технологические шоки на Шаге 10\n",
    "        env.stochastic_step()\n",
    "\n",
    "        # Обновляем шаг и проверяем условие остановки игры\n",
    "        env.current_episode += 1\n",
    "        \n",
    "        if env.current_episode > env.episodes_horizon:\n",
    "            done = True      \n",
    "\n",
    "    return \n",
    "\n",
    "### Для обуения большого числа эпох\n",
    "#for epoch in range(200):\n",
    "#    generate_session(env_)\n",
    "\n",
    "generate_session(env_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
